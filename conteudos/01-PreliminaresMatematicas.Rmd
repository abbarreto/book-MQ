# (PART) Tópicos Introdutórios {-}

\providecommand{\xmapsto}[1]{\overset{#1}{\longmapsto}}
\providecommand{\bra}[1]{\langle#1|}
\providecommand{\ket}[1]{|#1\rangle}
\providecommand{\braket}[2]{\langle#1|#2\rangle}
\providecommand{\proj}[1]{|#1\rangle\langle#1|}
\providecommand{\av}[1]{\langle#1\rangle}
\providecommand{\tr}{\operatorname{tr}}
\providecommand{\id}{\mathbf{1}}
\providecommand{\diag}[2]{\begin{bmatrix}#1&0\\0&#2\end{bmatrix}}
\providecommand{\mqty}[1]{\begin{matrix}#1\end{matrix}}
\providecommand{\bmqty}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\coloneqq}{\mathrel{=}}

# Preliminares matemáticas {#preliminares-matematicas}

Here we quickly recall most of the fundamental mathematical results that we will rely on in the rest of this book, most importantly *linear algebra over the complex numbers*.
However, we will not introduce everything from the ground up.
Most notably, we will assume that the reader understands what a **matrix** is, and how it represents a **linear transformation**; some prior exposure to **complex numbers** would be helpful.

If an equation like $\tr\ket{a}\bra{b}=\braket{b}{a}$ makes sense to you, then you can safely skip over this section and get started directly with Chapter \@ref(quantum-interference).

As a small note on notation, we generally write "$x\coloneqq y$" to mean "$x$ is defined to be (equal to) $y$", and "$x\equiv y$" to mean "$x$ is just another name for $y$", but sometimes we simply just write "$x=y$".


## Complex numbers

One of the fundamental ingredients of quantum information science (and, indeed, of quantum physics in general) is the notion of **complex numbers**.
It would be disingenuous to expect that a few paragraphs would suffice to make the reader sufficiently familiar with subject, but we try our best here to give a speedy overview of the core principles, and end with some exercises that can be a helpful indicator as to what things you might want to read up on elsewhere.

The "classical" way of arriving at complex numbers is as follows: start with the **natural numbers** $\mathbb{N}=\{0,1,2,\ldots\}$, which we can add; if we want to be able to invert addition (i.e. subtract), then we end up with the **integers** $\mathbb{Z}=\{\ldots,-2,-1,0,1,2,\ldots\}$, which we can multiply; if we want to be able to invert multiplication (i.e. divide), then we end up with the **rationals** $\mathbb{Q}=\{\frac{p}{q}\mid p,q\in\mathbb{Z}\}$.
In this process of "closure under more and more binary operations", we have passed from a **monoid**, to a **group**, to a **field**.
Algebraically, then, we seem to be done: we can do all the addition and multiplication that we like, and we can invert it whenever it makes sense to do so (e.g. we can divide, as long as it's not by $0$).

But there are lots of numbers that turn up in geometry that are not rational, such as $\sqrt{2}\approx1.414$, $\pi\approx3.14$, and $e\approx2.718$.
To include all of these (and simultaneously make sense of things like infinite sums, and limits), we must do some **real analysis** --- something which we won't touch upon here --- to end up with the **real numbers** $\mathbb{R}$.
These form a field, just like the rationals, but now we don't have any "gaps" in our number line.
So what's left to do?

Well the reals have one big problem: they are not **algebraically closed**.
That is, there exist polynomials with no roots, i.e. equations of the form $a_nx^n+a_{n-1}x^{n-1}+\ldots+a_1x+a_0=0$ (where the $a_i$ are real numbers) that have no solutions.^[To explain why we care so much about polynomials would be the subject of a whole nother book, but one important reason (of the *many*!), for both analysts and geometers alike, is the [**Weierstrass Approximation Theorem**](https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem).]
Somehow the most fundamental such example is the equation $x^2+1=0$, which has no solutions, because the square of any real number must be non-negative, and so $\sqrt{-1}\not\in\mathbb{R}$.

It turns out that if we just throw in this one extra number $i\coloneqq\sqrt{-1}$ to $\mathbb{R}$ then we can solve *any* polynomial --- a theorem so important that it's known as the **fundamental theorem of algebra**.
We call the result of doing this the **complex numbers**, and denote them by $\mathbb{C}$.

This gives us an algebraic way of understanding what a complex number is: it is a real number $x$ plus an **imaginary** number $iy$ (where $y\in\mathbb{R}$)
That is, every complex number $x+iy$ simply corresponds to a pair of real numbers $(x,y)$.
So now we can think geometrically!
We imagine the complex numbers $\mathbb{C}$ as the 2-dimensional Euclidean space $\mathbb{R}^2$, where the $x$-axis corresponds to the real part of a complex number, and the $y$-axis to the imaginary part.
This really is a geometric way of thinking, since now addition (and subtraction) of complex numbers (which is defined by adding their real and imaginary parts separately) is given by vector addition, as shown in Figure \@ref(fig:complex-addition).

(ref:complex-addition-caption) Addition of two complex numbers $z=x+iy$ and $z'=x'+iy'$, where we write $\Re$ (resp. $\Im$) for the real (resp. imaginary) part of a complex number: $\Re(x+iy)=x$ and $\Im(x+iy)=y$. Commutativity of addition corresponds to what is sometimes called the **parallelogram law** for addition of vectors.

But what about multiplication and division?
Following the rules of the game, we can figure out what the product of two complex numbers is by treating the imaginary number $i$ as a "formal variable", i.e. pretending it's just a variable in some polynomial, and then remembering that $i=\sqrt{-1}$ at the very end:
$$
  \begin{aligned}
    (x+iy)(x'+iy')
    &= xx'+ixy'+iyx'+i^2yy'
  \\&= xx'+ixy'+iyx'-yy'
  \\&= xx'-yy'+i(xy'+yx').
  \end{aligned}
$$

Division works similarly --- the most simple example of inverting a complex number $x+iy$ makes sense whenever $x$ and $y$ are both non-zero, since then we can use the trick of "multiplying by $1$":
$$
  \begin{aligned}
    \frac{1}{x+iy}
    &= \frac{1}{x+iy}\frac{x-iy}{x-iy}
  \\&= \frac{x-iy}{x^2+y^2}
  \\&= \frac{x}{x^2+y^2}-i\frac{y}{x+2+y^2}
  \end{aligned}
$$

This other complex number $x-iy$ that we used is somehow special because it is exactly the thing we needed to make the denominator real, so we give it a name: the **complex conjugate**^[The more common notation in mathematics is $\bar{z}$ instead of $z^\star$, but physicists tend to like the latter.] of a complex number $z=x+iy$ is the complex number $z^\star\coloneqq x-iy$.
Geometrically, this is just the reflection of the vector $(x,y)\in\mathbb{R}^2$ in the $x$-axis.
The product $zz^\star=x^2+y^2$ is also important: you might recognise (from Pythagoras' theorem) that $\sqrt{x^2+y^2}$ is exactly the length of the vector $(x,y)$, and so we call the real number $|z|\coloneqq\sqrt{zz^\star}$ the **modulus** (or magnitude, norm, or absolute value).
Note then that we can simply write $1/z=z^\star/|z|^2$.

Now things are looking somewhat nice, but the story isn't complete.
We have a good geometric intuition for what a complex number is (a vector in $\mathbb{R}^2$) and how to add them (vector addition), as well as what the complex conjugate and the modulus mean (reflection in the $x$-axis, and the length of the vector, respectively); but what about multiplication and division?

To understand these we need to switch from our **rectangular coordinates** $z=x+iy$ to **polar coordinates** --- instead of describing a point $z$ in $\mathbb{R}^2$ as "$x$ units left/right and $y$ units up/down", we describe it as "$r$ units from the origin, at an angle of $\theta$ [**radians**](https://en.wikipedia.org/wiki/Radian)".
We already know, given $(x,y)\in\mathbb{R}^2$, how to calculate its distance $r$ from the origin, since this is exactly the length of the vector: $r=|(x,y)|=\sqrt{x^2+y^2}$.
But what about the angle?
Some trigonometry tells us that $\theta=\tan(y/x)$, so we now know how to convert rectangular to polar coordinates:
$$
  x+iy = (x,y)
  \longmapsto (r,\theta) \coloneqq (\sqrt{x^2+y^2},\tan(y/x)).
$$

It would be nice to know how to go in the other direction though, but this can also be solved with some trigonometry:
$$
  (r,\theta)
  \longmapsto (r\cos\theta,r\sin\theta).
$$

(ref:converting-planar-and-polar-caption) Expressing a complex number $z$ in both planar and polar forms.

Great!
... but what's the point of polar coordinates?
Well, it turns out that they give us a geometric way of understanding multiplication: you can show^[**Exercise.** Prove this!] that $(r,\theta)$ multiplied by $(r',\theta')$ is exactly $(rr',\theta+\theta')$, which says that multiplication by a complex number $(r,\theta)$ is exactly a scaling by a factor of $r$ and a rotation by $\theta$.
This means that we can also easily find the multiplicative inverse of $(r,\theta)$, since it's just $(1/r,-\theta)$.
Finally, complex conjugation just means switching the sign of the angle: $(r,\theta)^\star=(r,-\theta)$.

There is one last ingredient that we should mention, which is the thing that really solidifies the relation between rectangular and polar coordinates.
We know that rectangular coordinates $(x,y)$ can be written as $x+iy$, so is there some more algebraic way of writing polar coordinates $(r,\theta)$?
Then we can avoid any ambiguity that might arise from using pairs of numbers --- if I tell you that I'm thinking of the complex number $z=(0.3,2)$, do I mean the point $0.3+2i$, or the point that is distance $r$ from the origin at an angle of $2$ radians?

Given polar coordinates $(r,\theta)$, we know that this is equal to $(r\cos\theta,r\sin\theta)$ in rectangular coordinates.
For simplicity, let's first consider the case where $r=1$.
Then we can write $(1,\theta)$ as $\cos\theta+i\sin\theta$.
Using the [**Taylor series**](https://en.wikipedia.org/wiki/Taylor_series)^[If you don't know about Taylor series, then feel free to just skim this part, but make sure to read the punchline!] of $\sin$ and $\cos$, we can rewrite this as
$$
  \begin{aligned}
    \cos\theta+i\sin\theta
    &= \left(
      1-\frac{\theta^2}{2!}+\frac{\theta^4}{4!}-\ldots
    \right) + i\left(
      \theta-\frac{\theta^3}{3!}+\frac{\theta^5}{5!}-\ldots
    \right)
  \\&= 1+i\theta-\frac{\theta^2}{2!}-i\frac{\theta^3}{3!}+\frac{\theta^4}{4!}+i\frac{\theta^5}{5!}-\ldots
  \\&= 1+i\theta+\frac{i^2\theta^2}{2!}+\frac{i^3\theta^3}{3!}+\frac{i^4\theta^4}{4!}+\frac{i^5\theta^5}{5!}+\ldots
  \\&= \exp(i\theta)
  \end{aligned}
$$
where at the very end we use the Taylor expansion of the [**exponential function**](https://en.wikipedia.org/wiki/Exponential_function) $\exp(x)=e^x$.

We have just "proved"^[It is very important to point out that this "proof" is not rigorous or formal --- you need to be very *very* careful when rearranging infinite sums! However, this proof *can be made rigorous* by using some real analysis.] one of the most remarkable formulas in mathematics: **Euler's formula**
$$
  e^{i\theta} = \cos\theta+i\sin\theta
$$
(a special case of which gives the famous equation $e^{i\pi}+1=0$, uniting five fundamental constants: $0$, $1$, $i$, $e$, and $\pi$).
In summary then, we have two beautiful ways of expressing a complex number $z\in\mathbb{C}$, in either its rectangular/planar form or its polar/Euler form:
$$
  z = x+iy = re^{i\theta}.
$$

::: {.idea latex=""}
Addition and subtraction are most neatly expressed in the planar form $x+iy$, and multiplication and division are most neatly expressed in the polar form $re^{i\theta}$; complex conjugation looks nice and tidy in both.
:::

::: {.technical title="Addition of polar vectors" latex=""}
We know how to perform addition, multiplication, inversion (which is a special case of division), and complex conjugation on complex numbers in planar form, but we've only described how to do the last *three* of these in polar form: we haven't said how to write $re^{i\theta}+r'e^{i\theta'}$ as $se^{i\varphi}$ for some $s$ and $\varphi$.
This is because it is very messy looking:
$$
  \begin{aligned}
    s
    &= \sqrt{r^2+(r')^2+2rr'\cos(\theta'-\theta)}
  \\\varphi
    &= \theta+\operatorname{atan2}\big(r'\sin(\theta'-\theta),r+r'\cos(\theta'-\theta)\big)
  \end{aligned}
$$
and where $\operatorname{atan2}$ is the [2-argument arctangent function](https://en.wikipedia.org/wiki/Atan2).
:::

You do not need to know everything about this whole story of algebraically closed fields and so on, but it helps to know the basics, so here are some exercises that should help you to become more familiar.^[Note that we have not really given you enough information in this section to be able to solve all these exercises, but that is intentional! Sometimes we like to ask questions and not answer them, with the hope that you will enjoy getting to do some research by yourself.]

a. The set $\mathbb{Q}$ of rational numbers and the set $\mathbb{R}$ of real numbers are both fields, but the set $\mathbb{Z}$ of integers is not. Why not?
b. Look up the formal statement of the fundamental theorem of algebra.
c. Evaluate each of the following quantities:
  $$
    1+e^{-i\pi},
    \quad
    |1+i|,
    \quad
    (1+i)^{42},
    \quad
    \sqrt{i},
    \quad
    2^i,
    \quad
    i^i.
  $$
d. Here is a simple "proof" that $+1=-1$:
  $$
    1=\sqrt{1}=\sqrt{(-1)(-1)}=\sqrt{-1}\sqrt{-1}=i^2=-1.
  $$
  What is wrong with it?
e. Prove that, for any two complex numbers $w,z\in\mathbb{C}$, we always have the inequality
  $$
    |z-w| \geq |z|-|w|.
  $$
  *Hint: use polar form, draw a diagram, and appeal to the [**triangle inequality**](https://en.wikipedia.org/wiki/Triangle_inequality)!*
f. Using the fact that $e^{3i\theta}=(e^{i\theta})^3$, derive a formula for $\cos3\theta$ in terms of $\cos\theta$ and $\sin\theta$.


## Euclidean vectors and vector spaces

We assume that you are familiar with Euclidean vectors --- those arrow-like geometric objects which are used to represent physical quantities, such as trajectories, velocities, or forces.
You know that any two velocities can be added to yield a third, and the multiplication of a "velocity vector" by a real number is another "velocity vector".
So a **linear combination** of vectors is another vector: if $v$ and $w$ are vectors, and $\lambda$ and $\mu$ are numbers (rational, real, or complex, for example), then $\lambda v+\mu w$ is another vector.
Mathematicians have simply taken these properties and defined vectors as _anything_ that we can add and multiply by numbers, as long as everything behaves in a nice enough way.
This is basically what an Italian mathematician Giuseppe Peano (1858--1932) did in a chapter of his 1888 book with an impressive title: _Calcolo geometrico secondo l'Ausdehnungslehre di H. Grassmann preceduto dalle operazioni della logica deduttiva_.
Following Peano, we define a **vector space** as a mathematical structure in which the notion of linear combination "makes sense".

More formally, a **complex vector space** is a set $V$ such that, given any two **vectors** $a$ and $b$ (that is, any two elements of $V$) and any two *complex* numbers $\alpha$ and $\beta$, we can form the linear combination $\alpha a+\beta b$, which is also a vector in $V$.
There are certain "nice properties" that vector spaces things must satisfy. Addition of vectors must be commutative and associative, with an identity (the zero vector, which is often written as $\mathbf{0}$) and an inverse for each $v$ (written as $-v$). Multiplication by complex numbers must obey the two distributive laws: $(\alpha+\beta)v = \alpha v+\beta v$ and $\alpha (v+w) = \alpha v+\alpha w$.

::: {.technical title="Modules over a ring" latex=""}
A more succinct way of defining a vector space is as an abelian group endowed with a scalar action of a field.
This showcases vector spaces as a particularly well behaved example of a more general object: **modules over a ring**.
:::

A **subspace** of $V$ is any subset of $V$ which is closed under vector addition and multiplication by complex numbers.
Here we start using the Dirac bra-ket notation and write vectors in a somewhat fancy way as $\ket{\text{label}}$, where the "label" is anything that serves to specify what the vector is.
For example, $\ket{\uparrow}$ and $\ket{\downarrow}$ may refer to an electron with spin up or down along some prescribed direction, and $\ket{0}$ and $\ket{1}$ may describe a quantum bit holding either logical $0$ or $1$.
As a maybe more familiar example, the set of binary strings of length $n$ is a vector space over the field $\mathbb{Z}/2\mathbb{Z}$ of integers mod $2$; in the case $n=2$ we can write down all the vectors in this vector space in this notation: $\ket{00}$, $\ket{01}$, $\ket{10}$, $\ket{11}$, where e.g. $\ket{10}+\ket{11}=\ket{01}$ (addition is taken mod $2$).
These are often called **ket** vectors, or simply **kets**.
(We will deal with "bras" in a moment).

A **basis** in $V$ is a collection of vectors $\ket{e_1},\ket{e_2},\ldots,\ket{e_n}$ such that every vector $\ket{v}$ in $V$ can be written (in _exactly_ one way) as a linear combination of the basis vectors: $\ket{v}=\sum_{i=1}^n v_i\ket{e_i}$.
The number of elements in a basis is called the **dimension** of $V$.^[Showing that this definition is independent of the basis that we choose is a "fun" linear algebra exercise.]
The most common, and prototypical, $n$-dimensional complex vector space (and the space which we will be using most of the time) is the space of ordered $n$-tuples of complex numbers, usually written as column vectors:
$$
  \ket{a}
  = \begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}
$$
with a basis given by the column vectors $\ket{e_i}$ that are $0$ in every row except for a $1$ in the $i$-th row:
$$
  \ket{e_1}
  = \begin{bmatrix}1\\0\\\vdots\\0\end{bmatrix}
  \qquad
  \ket{e_2}
  = \begin{bmatrix}0\\1\\\vdots\\0\end{bmatrix}
  \qquad\ldots\qquad
  \ket{e_n}
  = \begin{bmatrix}0\\0\\\vdots\\1\end{bmatrix}
$$
and where addition of vectors is done **component-wise**, so that
$$
  \left(\sum_{i=1}^n v_i\ket{e_i}\right)+\left(\sum_{i=1}^n w_i\ket{e_i}\right)
  = \sum_{i=1}^n (v_i+w_i)\ket{e_i}
$$
or, in column vectors,
$$
  \begin{gathered}
    \ket{v}
    = \begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}
    \qquad
    \ket{w}
    = \begin{bmatrix}w_1\\w_2\\\vdots\\w_n\end{bmatrix}
  \\\alpha\ket{a}+\beta\ket{b}
    = \begin{bmatrix}\alpha v_1+\beta w_1\\\alpha v_2+\beta w_2\\\vdots\\\alpha v_n+\beta w_n\end{bmatrix}
  \end{gathered}
$$

Throughout the course we will deal only with vector spaces of *finite* dimensions.
This is sufficient for all our purposes and we will avoid many mathematical subtleties associated with infinite dimensional spaces, for which we would need the tools of **functional analysis**.

Formally, whenever we say **$n$-dimensional Euclidean space**, we mean the *real* vector space $\mathbb{R}^n$.


## Bras and kets {#bras-and-kets}

An **inner product** on a vector space $V$ (over the complex numbers) is a function that assigns to each pair of vectors $\ket{u},\ket{v}\in V$ a complex number $\braket{u}{v}$, and satisfies the following conditions:

- $\braket{u}{v}=\braket{v}{u}^\star$
- $\braket{v}{v}\geq 0$ for all $\ket{v}$
- $\braket{v}{v}= 0$ if and only if $\ket{v}=0$

where ${}^\star$ denotes complex conjugation (sometimes written as $z\mapsto\bar{z}$ instead).

The inner product must also be _linear_ in the second argument but _antilinear_ in the first argument:
$$
  \braket{c_1u_1+c_2u_2}{v} = c_1^\star\braket{u_1}{v}+c_2^\star\braket{u_2}{v}
$$
for any complex constants $c_1$ and $c_2$.

To any physical system we associate^[The question of *how* exactly we construct this associated space is a subtle one in the case of arbitrary physical systems, but we shall see that this is relatively straightforward when working with the types of systems that we consider in this book.] a complex vector space with an inner product, known as a **Hilbert space** $\mathcal{H}$.
The inner product between vectors $\ket{u}$ and $\ket{v}$ in ${\mathcal{H}}$ is written as $\braket{u}{v}$.

::: {.technical title="Finite-dimensional functional analysis" latex=""}
If $V$ is a vector space with an inner product $\langle-,-\rangle$, then this gives us a **norm** on $V$ by defining $\|x\|=\sqrt{\langle x,x\rangle}$ and thus a **metric** by defining $d(x,y)=\|y-x\|$.
We say that a sequence $(x_n)$ in $V$ is **Cauchy** if its elements "eventually always get closer", i.e. if for all $\varepsilon>0$ there exists some $N\in\mathbb{N}$ such that for all $m,n>N$ we have $\|x_n-x_m\|<\varepsilon$.
We say that a normed space is **complete** if *every Cauchy sequence converges in that space*, i.e. if the limits of sequences that *should* exist actually *do* exist.

Now one useful fact is the following: on a *finite dimensional* vector space, all norms are equivalent.
(Note that this does *not* mean that $\|x\|_1=\|x\|_2$ for any two norms $\|-\|_1$ and $\|-\|_2$, but simply that they "induce the same topology" --- feel free to look up the precise definition elsewhere).
This follows from another useful fact: in a *finite dimensional* vector space, the unit ball is compact.
By a short topological argument, we can use these facts to show that what we claimed, namely that every *finite dimensional* inner product space is complete (with respect to the norm induced by the inner product, and thus with respect to *any* norm, since all norms are equivalent).

In the infinite dimensional case these facts are *not* true, and we have a special name for those inner product spaces which *are* complete: **Hilbert spaces**.
So working in the finite dimensional case means that "we do not have to worry about analysis", in that the completeness property comes for free the moment we have an inner product, and we can freely refer to inner product spaces as Hilbert spaces.
:::

For example, for column vectors $\ket{u}$ and $\ket{v}$ in $\mathbb{C}^n$ written as
$$
  \ket{u}
  = \begin{bmatrix}u_1\\u_2\\\vdots\\u_n\end{bmatrix}
  \qquad
  \ket{v}
  = \begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}
$$
their inner product is defined by
$$
  \braket{u}{v}
  = u_1^\star v_1 + u_2^\star v_2+\ldots + u_n^\star v_n.
$$
Following Dirac, we may split the inner product into two ingredients:
$$
  \braket{u}{v}
  \longrightarrow \bra{u}\,\ket{v}.
$$
Here $\ket{v}$ is a ket vector, and $\bra{u}$ is called a **bra** vector, or a **bra**, and can be represented by a row vector:
$$
  \bra{u}
  = \begin{bmatrix}u_1^\star,&u_2^\star,&\ldots,&u_n^\star\end{bmatrix}.
$$
The inner product can now be viewed as the result of the matrix multiplication:
$$
  \begin{aligned}
    \braket{u}{v}
    &= \begin{bmatrix}u_1^\star,&u_2^\star,&\ldots,&u_n^\star\end{bmatrix}
    \cdot \begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}
  \\&= u_1^\star v_1 + u_2^\star v_2 + \ldots + u_n^\star v_n.
  \end{aligned}
$$

Bras are vectors: you can add them, and multiply them by scalars (which, here, are complex numbers), but they are vectors in the space ${\mathcal{H}}^\star$ which is **dual** to $\mathcal{H}$.
Elements of ${\mathcal{H}}^\star$ are **linear functionals**, that is, linear maps from $\mathcal{H}$ to $\mathbb{C}$.
A linear functional $\bra{u}$ acting on a vector $\ket{v}$ in $\mathcal{H}$ gives a complex number $\braket{u}{v}$.

::: {.idea latex=""}
All Hilbert spaces of the same (finite) dimension are isomorphic, so the differences between quantum systems cannot be really understood without additional structure. This structure is provided by a specific algebra of operators acting on $\mathcal{H}$.
:::


## Daggers

Although $\mathcal{H}$ and $\mathcal{H}^\star$ are not identical spaces --- the former is inhabited by kets, and the latter by bras --- they are closely related.
There is a bijective map from one to the other given by $\ket{v}\leftrightarrow \bra{v}$, and denoted by a **dagger**:^["Is this a $\dagger$ which I see before me..."]
$$
  \begin{aligned}
    \bra{v}
    &= (\ket{v})^\dagger
  \\\ket{v}
    &= (\bra{v})^\dagger.
  \end{aligned}
$$
We usually omit the parentheses when it is obvious what the dagger operation applies to.

The dagger operation, also known as **Hermitian conjugation**, is _antilinear_:
$$
  \begin{aligned}
    (c_1\ket{v_1}+c_2\ket{v_2})^\dagger
    &= c_1^\star\bra{v_1} + c_2^\star\bra{v_2}
  \\(c_1\bra{v_1}+c_2\bra{v_2})^\dagger
    &= c_1^\star\ket{v_1} + c_2^\star\ket{v_2}.
  \end{aligned}
$$
Also, when applied twice, the dagger operation is the identity map.

You might already be familiar with Hermitian conjugation under another name: the **conjugate transpose**^[In mathematics texts this operation is often denoted by ${}^\star$ rather than ${}^\dagger$, but we reserve the former for complex conjugation *without* matrix transposition. Note, however, that scalars can be thought of as $(1\times1)$ matrices, and in this special case we have that $\dagger=\star$.] of an $(n\times m)$ matrix $A$ is an $(m\times n)$ matrix $A^\dagger$, obtained by interchanging the rows and columns of $A$ and taking complex conjugates of each entry in $A$, i.e. $A^\dagger_{ij}=A^\star_{ji}$.
In particular then,
$$
  \ket{v} = \begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}
  \overset{\dagger}{\longleftrightarrow}
  \bra{v} = \begin{bmatrix}v_1^\star,&v_2^\star,&\ldots,&v_n^\star\end{bmatrix}.
$$



## Geometry {#geometry}

The inner product brings geometry: the **length**, or **norm**, of $\ket{v}$ is given by $\|v\|=\sqrt{\braket{v}{v}}$, and we say that $\ket{u}$ and $\ket{v}$ are **orthogonal** if $\braket{u}{v}=0$.
Any maximal set of pairwise orthogonal vectors of unit length^[That is, consider sets of vectors $\ket{e_i}$ such that $\braket{e_i}{e_j}=\delta_{ij}$ (where the **Kronecker delta** $\delta_{ij}$ is $0$ if $i\neq j$, and $1$ if $i=j$.), and then pick any of the largest such sets (which must exist, since we assume our vector spaces to be finite dimensional).] forms an orthonormal basis, and so any vector can be expressed as a linear combination of the basis vectors:
$$
  \begin{gathered}
    \ket{v}
    =\sum_i v_i\ket{e_i}
  \\\text{where $v_i=\braket{e_i}{v}$}.
  \end{gathered}
$$
Then the bras $\bra{e_i}$ form the **dual basis**
$$
  \begin{gathered}
    \bra{v}
    =\sum_i v_i^\star\bra{e_i}
  \\\text{where $v_i^\star=\braket{v}{e_i}$}.
  \end{gathered}
$$

To make the notation a bit less cumbersome, we will sometimes label the basis kets as $\ket{i}$ rather than $\ket{e_i}$, and write
$$
  \begin{aligned}
    \ket{v}
    &= \sum_i \ket{i}\braket{i}{v}
  \\\bra{v}
    &= \sum_j \braket{v}{i}\bra{i}
  \end{aligned}
$$
but *do not confuse $\ket{0}$ with the zero vector*!
We *never* write the zero vector as $\ket{0}$, but only ever as $0$, without any bra or ket decorations (so e.g. $\ket{v}+0=\ket{v}$).

Now that we have some notion of geometry, we can explain a bit more about this idea of associating a Hilbert space to a quantum system --- we will use some terminology that we have not yet introduced, but all will be explained in due time.

::: {.idea latex=""}
To any *isolated* quantum system, which can be prepared in $n$ **perfectly distinguishable** states, we can associate a Hilbert space $\mathcal{H}$ of dimension $n$ such that each vector $\ket{v}\in\mathcal{H}$ of unit length $\braket{v}{v}=1$ represents a quantum state of the system.
The overall phase of the vector has no physical significance: $\ket{v}$ and $e^{i\varphi}\ket{v}$ (for any real $\varphi$) both describe the same state.
:::

We note here one more fact that also won't yet make sense, but which won't hurt to have hidden away in the back of your mind.

::: {.idea latex=""}
The inner product $\braket{u}{v}$ is the **probability amplitude** that a quantum system prepared in state $\ket{v}$ will be found in state $\ket{u}$ upon measurement.
This means that states corresponding to orthogonal vectors (i.e. $\braket{u}{v}=0$) are perfectly distinguishable: if we prepare the system in state $\ket{v}$, then it will never be found in state $\ket{u}$, and vice versa.
:::


## Operators

A **linear map** between two vector spaces $\mathcal{H}$ and $\mathcal{K}$ is a function $A\colon\mathcal{H}\to\mathcal{K}$ that respects linear combinations:
$$
  A(c_1\ket{v_1}+c_2\ket{v_2})=c_1 A\ket{v_1}+c_2 A\ket{v_2}
$$
for any vectors $\ket{v_1},\ket{v_2}$ and any complex numbers $c_1,c_2$.
We will focus mostly on **endomorphisms**, that is, maps from $\mathcal{H}$ to $\mathcal{H}$, and we will call them **operators**.
The symbol $\id$ is reserved for the identity operator that maps every element of $\mathcal{H}$ to itself (i.e. $\id\ket{v}=\ket{v}$ for all $\ket{v}\in\mathcal{H}$).
The product $BA$ of two operators $A$ and $B$ is the operator obtained by first applying $A$ to some ket $\ket{v}$ and then $B$ to the ket which results from applying $A$:
$$
  (BA)\ket{v} = B(A\ket{v}).
$$
The order _does_ matter: in general, $BA\neq AB$.
In the exceptional case in which $AB=BA$, one says that these two operators **commute**.
The inverse of $A$, written as $A^{-1}$, is the operator that satisfies $AA^{-1}=\id=A^{-1}A$.
For finite-dimensional spaces, one only needs to check _one_ of these two conditions, since any one of the two implies the other, whereas, on an infinite-dimensional space, _both_ must be checked.
Finally, given a particular basis, an operator $A$ is uniquely determined by the entries of its matrix: $A_{ij}=\bra{i}A\ket{j}$.

The **adjoint**, or **Hermitian conjugate**, of an linear map $A$, denoted by $A^\dagger$, is defined by the relation
$$
  \begin{gathered}
    \bra{i}A^\dagger\ket{j}
    = \bra{j}A\ket{i}^\star
  \\\text{for all $\ket{i},\ket{j}\in\mathcal{H}$}
  \end{gathered}
$$
and $\dagger$ turns $(n\times m)$ matrices into $(m\times n)$ matrices.

An operator $A$ is said to be

- **normal** if $AA^\dagger = A^\dagger A$
- **unitary** if $A^\dagger=A^{-1}$
- **Hermitian** (or **self-adjoint**) if $A^\dagger = A$.

In particular then, being unitary implies being normal, since if $A^\dagger=A^{-1}$ then $AA^\dagger=A^\dagger A$, since both of these are equal to $\id$.
Note also that unitary and Hermitian operators must be operators, i.e. they are represented by an $(n\times n)$ matrix.

Any *physically admissible* evolution of an isolated quantum system is represented by a unitary operator.^[This is an *axiom*, justified by experimental evidence, and also by some sort of mathematical intuition. So, in this book, we take this as a *fact* that we do not question. It is, however, very interesting to question it: *why should we assume this to be true?*]
Note that unitary operators preserve the inner product: given a unitary operator $U$ and two kets $\ket{a}$ and $\ket{b}$, and defining $\ket{a'}=U\ket{a}$ and $\ket{b'}=U\ket{b}$, we have that
$$
  \begin{gathered}
    \bra{a'}=\bra{a}U^\dagger
  \\\bra{b'}=\bra{b}U^\dagger
  \\\braket{a'}{b'}=\bra{a}U^\dagger U\ket{b}=\bra{a}\id\ket{b}=\braket{a}{b}.
  \end{gathered}
$$
Preserving the inner product implies preserving the norm induced by this product, i.e. unit state vectors are mapped to unit state vectors, i.e. _unitary operations are the isometries of the Euclidean norm_.


## Eigenvalues and eigenvectors

Given an operator $A$, an **eigenvector** is a non-zero vector $\ket{v}$ such that
$$
  A\ket{v} = \lambda\ket{v}
$$
for some $\lambda\in\mathbb{C}$ (which is called the corresponding **eigenvalue**).
We call the pair $(\lambda,\ket{v})$ an **eigenpair**, and we call the set of eigenvalues the **spectrum** of $A$, denoted by $\sigma(A)$.
It is a surprising (but incredibly useful) fact that every operator has at least one eigenpair.^[You can prove this for an $(n\times n)$ matrix $A$ by considering the set $\{\ket{v},A\ket{v},A^2\ket{v},\ldots,A^n\ket{v}\}$ of vectors in $\mathbb{C}^n$. Since this has $n+1$ elements, it must be linearly *dependent*, and so (after some lengthy algebra) we can construct an eigenpair.]
Geometrically, an eigenvector of an operator $A$ is a vector upon which $A$ simply acts by "stretching".

Rewriting the defining property of an eigenpair $(\lambda,\ket{v})$, we see that
$$
  (A-\lambda\id)\ket{v} = 0
$$
which tells us that the operator $A-\lambda\id$ has a non-zero kernel, and is thus non-invertible.
This gives a useful characterisation of the spectrum in terms of a determinant:
$$
  \sigma(A) = \{\lambda\in\mathbb{C} \mid \det(A-\lambda\id)=0\}.
$$


## Outer products {#outer-products}

Apart from the inner product $\braket{u}{v}$, which is a complex number, we can also form the **outer product** $\ket{u}\bra{v}$, which is a linear map (operator) on $\mathcal{H}$ (or on $\mathcal{H}^\star$, depending how you look at it).
This is what physicists like (and what mathematicians dislike!) about Dirac notation: a certain degree of healthy ambiguity.

- The result of $\ket{u}\bra{v}$ acting on a ket $\ket{x}$ is $\ket{u}\braket{v}{x}$, i.e. the vector $\ket{u}$ multiplied by the complex number $\braket{v}{x}$.
- Similarly, the result of $\ket{u}\bra{v}$ acting on a bra $\bra{y}$ is $\braket{y}{u}\bra{v}$, i.e. the linear functional $\bra{v}$ multiplied by the complex number $\braket{y}{u}$.

The product of two maps, $A=\ket{a}\bra{b}$ followed by $B=\ket{c}\bra{d}$, is a linear map $BA$, which can be written in Dirac notation as
$$
  BA = \ket{c}\braket{d}{a}\bra{b} = \braket{d}{a}\ket{c}\bra{b}
$$
i.e. the inner product (complex number) $\braket{d}{a}$ times the outer product (linear map) $\ket{c}\bra{b}$.

Any operator on $\mathcal{H}$ can be expressed as a sum of outer products. Given an orthonormal basis $\{\ket{e_i}\}_{i=1,\ldots,n}$, any operator which maps the basis vectors $\ket{e_i}$ to vectors $\ket{f_i}$ can be written as $\sum_{i=1}^n\ket{f_i}\bra{e_i}$.
If the vectors $\{\ket{f_i}\}$ *also* form an orthonormal basis then the operator simply "rotates" one orthonormal basis into another.
These are unitary operators which preserve the inner product.
In particular, if each $\ket{e_i}$ is mapped to $\ket{e_i}$, then we obtain the identity operator:
$$
  \sum_i\ket{e_i}\bra{e_i}=\id.
$$ 
This relation holds for _any_ orthonormal basis, and it is one of the most ubiquitous and useful formulas in quantum theory, known as **completeness**.^[Not to be confused with "completeness" in the sense of Hilbert spaces.]
For example, for any vector $\ket{v}$ and for any orthonormal basis $\{\ket{e_i}\}$, we have
$$
  \begin{aligned}
    \ket{v}
    &= \id\ket{v}
  \\&= \sum_i \ket{e_i}\bra{e_i}\;\ket{v}
  \\&= \sum_i \ket{e_i}\;\braket{e_i}{v}
  \\&= \sum_i v_i\ket{e_i}
  \end{aligned}
$$
where $v_i=\braket{e_i}{v}$ are the components of $\ket{v}$.

Finally, note that calculating the adjoint of an outer product boils down to just swapping the order:
$$
  (\ket{a}\bra{b})^\dagger = \ket{b}\bra{a}.
$$

::: {.technical title="Dagger compact categories" latex=""}
This whole package of stuff and properties and structure (i.e. finite dimensional Hilbert spaces with linear maps and the dagger) bundles up into an abstract framework called a [**dagger compact category**](https://en.wikipedia.org/wiki/Dagger_compact_category).
We will not delve into the vast world of category theory in this book, and to reach an understanding of all the ingredients that go into the one single definition of dagger compact categories would take more than a single chapter.
But it's a good idea to be aware that there are researchers in quantum information science who work *entirely* from this approach, known as [**categorical quantum mechanics**](https://en.wikipedia.org/wiki/Categorical_quantum_mechanics).
:::

## The trace

The **trace** is an operation which turns outer products into inner products,
$$
  \tr\colon \ket{b}\bra{a} \longmapsto \braket{a}{b}.
$$
We have just seen that any linear operator can be written as a sum of outer products, and so we can extend the definition of trace (by linearity) to any operator.
Equivalently, for any square matrix $A$, the trace of $A$ can be defined to be the sum of its diagonal elements:
$$
  \tr A = \sum_k \bra{e_k}A\ket{e_k} = \sum_k A_{kk}.
$$
In fact, the trace of $A$ is equal to the sum of the eigenvalues of $A$, even in the case where $A$ is not diagonalisable.

You can show, using this definition or otherwise, that the trace is cyclic^[Note that "cyclic" does not mean the same thing as "permutation invariant"! It is *not* true in general that $\tr(ABC)=\tr(CBA)$, but only that $\tr(ABC)=\tr(BCA)=\tr(CAB)$, i.e. we can only *cyclically* permute the operators.] ($\tr(AB) = \tr(BA)$) and linear ($\tr(\alpha A+\beta B) = \alpha\tr(A)+\beta\tr(B)$, where $A$ and $B$ are square matrices and $\alpha$ and $\beta$ complex numbers).

To recover the first definition from the second, we argue as follows:
$$
  \begin{aligned}
    \tr\ket{b}\bra{a}
    &= \sum_k \braket{e_k}{b}\braket{a}{e_k}
  \\&= \sum_k \braket{a}{e_k}\braket{e_k}{b}
  \\&= \bra{a}\id\ket{b}
  \\&= \braket{a}{b}.
  \end{aligned}
$$
Here, the second term can be viewed both as the sum of the diagonal elements of $\ket{b}\bra{a}$ in the $\ket{e_k}$ basis, and as the sum of the products of two complex numbers $\braket{e_k}{b}$ and $\braket{a}{e_k}$.
We have used the decomposition of the identity, $\sum_k\ket{e_k}\bra{e_k}=\id$.
Given that we can decompose the identity by choosing any orthonormal basis, it is clear that the trace does _not_ depend on the choice of the basis.


## Some useful identities {#some-useful-identities}

Here is a summary of some particularly useful equalities concerning bras, kets, inner products, outer products, traces, and operators, that we will be using time and time again.
In all of these, $\ket{a},\ket{b}\in\mathcal{H}$ are kets, $A,B,C$ are operators on $\mathcal{H}$, and $\alpha,\beta\in\mathbb{C}$ are scalars.

Dagger for bras and kets:

- $\ket{a}^\dagger = \bra{a}$
- $\bra{a}^\dagger = \ket{a}$
- $(\ket{a}\bra{b})^\dagger = \ket{b}\bra{a}$
- $(\alpha\ket{a}+\beta\ket{b})^\dagger = \alpha^\star\bra{a}+\beta^\star\bra{b}$

Dagger for operators:

- $(AB)^\dagger = B^\dagger A^\dagger$
- $(A^\dagger)^\dagger = A$
- $(\alpha A+\beta B)^\dagger = \alpha^\star A^\dagger+\beta^\star B^\dagger$

Trace:

- $\tr(\alpha A+\beta B) = \alpha \tr(A)+\beta\tr(B)$
- $\tr(ABC) = \tr(CAB) = \tr(BCA)$
- $\tr\ket{a}\bra{b} = \braket{b}{a}$
- $\tr (A\ket{a}\bra{b}) = \braket{b}{A|a} = \tr(\ket{a}\bra{b}A)$
